---
title: "Russia IRA 10m analysis"
author: "Alex Stephenson"
date: "02/09/2019"
output: html_document
---

```{r error=FALSE, message=FALSE, warning=FALSE}  
library(tidyverse)
library(lubridate)
library(tidytext)
library(tidyr)
library(tm)
library(widyr)
library(ggraph)
library(stm)
library(igraph)
library(quanteda)
library(rebus)
library(rvest)
library(remotes)
library(devtools)
library(tidylo)
library(widyr)
library(igraph)
library(ggraph)
set.seed(1234)

options(scipen = 999, digits = 10)

```


```{r}
data <- read_csv("ira_tweets_csv_hashed.csv")
```

```{r}
ru_stopwords <- tibble(stopwords("ru"))
colnames(ru_stopwords) <- "word"

custom_stop_words <- bind_rows(tibble(word = c("https", "t.co", "rt", "amp", "все", "это")), 
                               ru_stopwords) 


custom_stop_words <- bind_rows(tibble(word = c("https", "t.co", "rt", "amp", "все", "это", 'мной', 'почему', 'спасибо', 'просто', 'очень', "привет", "всё", "тебе", "вообще", "день", "думаю", "пока", "такое", "такие", "таких", "изза", "ещё", "знаю")),  
                               ru_stopwords) 


```

```{r}
tweets <- data %>% 
  filter(tweet_language == "ru" | account_language == "ru")
#tweets <- tweets %>% 
#  select(tweetid, userid, follower_count, following_count, tweet_text, tweet_time, like_count, retweet_count, quote_count, hashtags, in_reply_to_tweetid, #in_reply_to_userid, reply_count, account_creation_date)

```


user info 
```{r}
user_info <- read_csv('ira_users_csv_hashed.csv')
```

```{r}
ggplot(user_info, aes(account_creation_date)) + 
  geom_histogram(bins = 100)
```


```{r}
tweets %>% 
  select(reply_count) %>% 
  na.omit() %>% 
  filter(reply_count != 0) %>% 
  arrange(desc(reply_count)) %>%  
  head(n = 10)
```

```{r}
tweets %>%  
  select(reply_count) %>% 
  na.omit() %>% 
  summarise(total = sum(reply_count),
            mean = mean(reply_count),
            `% of tweets with a reply` = '4%')


```

Of the 787k tweets nearly a third were from other IRA accounts. 

```{r}
ggplot(tweets, aes(tweet_time)) +
  geom_histogram(bins = 100) +
  labs(x = "Date", y = "Number of tweets", title = "Number of tweets between 2010 and 2018")
```




```{r active_days}
most_active_days <- tweets %>% 
  mutate(tweet_time = floor_date(tweet_time, unit = "1 day")) %>% 
  count(tweet_time, sort = T) 

most_active_days %>% 
  mutate(tweet_time = reorder(tweet_time, n)) %>% 
  head(n = 10) %>% 
  ggplot(aes(tweet_time, n)) +
  geom_col() +
  coord_flip() +
  labs(title = "10 most 'active' days", y = "Number of tweets", x = "Date of tweet")
```



Lets see what words were being used on this day:
```{r}
most_active_day <- tweets %>% 
  mutate(tweet_time = floor_date(tweet_time, unit = "1 day")) %>% 
  filter(tweet_time == as.Date("2014-07-18"))

most_active_day_tokens <- most_active_day %>% 
  unnest_tokens(word, tweet_text, token = "tweets") %>% 
  anti_join(custom_stop_words, by = "word") 

most_active_day_tokens %>% 
  count(word, sort = T) %>% 
  filter(n > 2000) %>%
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(x = "", y = "Word", title = "Most common words on 6th October, 2016")
```


```{r}
most_active_days %>% 
  ggplot(aes(n)) +
  geom_histogram() +
  scale_x_log10() +
  labs(title = "Number of days with 'n' amount of tweets on log scale", y = "")
 
```

```{r popular_hashtags}
tweets %>% 
  count(hashtags, sort = T) %>% 
  filter(hashtags != "[]") %>% 
  filter(n > 5000) %>% 
  mutate(hashtags = reorder(hashtags, n)) %>% 
  ggplot(aes(x = hashtags, y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Most popular hashtags", x = "Hashtags", y = "Number of occurances")
```

How were the number of retweets and favourites distributed across the dataset?

```{r retweet_count, warning=FALSE, message=FALSE}

par(mfrow = c(1, 2))

tweets %>% 
  ggplot(aes(retweet_count)) +
  geom_histogram() +
  scale_x_log10() +
  labs(title = "Retweet count on log scale", y = "", x = "Number of retweets on each tweet")

```


```{r friend_count, warning = FALSE, message=FALSE}
user_info %>% 
  ggplot(aes(follower_count)) +
  geom_histogram() +
  scale_x_log10() +
  labs(caption = "Normal log distribution with left skew", x = "Friend count", title = "Distribution of 'friends' on a log scale")
#normal log distribution with left skew
```


```{r}
march_14 <- tweets %>% 
  filter(tweet_time > as.Date("2014-03-01") & tweet_time < as.Date("2014-04-01")) 

```


```{r}
tweets_2017 <- tweets %>% 
  filter(tweet_time > as.Date("2017-01-01") & tweet_time < as.Date("2018-01-01"))
  


bigrams <- tweets_2017 %>% 
  unnest_tokens(bigram, tweet_text, token = "ngrams", n = 2) #re-tokenise our tweets with bigrams. 

bigrams_separated <- bigrams %>% 
  separate(bigram, c("word_1", "word_2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word_1 %in% custom_stop_words$word) %>% 
  filter(!word_2 %in% custom_stop_words$word)

  
bigram_counts <- bigrams_filtered %>% 
  count(word_1, word_2 , sort = T)

bigram_graph <- bigram_counts %>% 
  filter(n > 500) %>% 
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

```{r}
tokens_2017 <- tweets_2017 %>% 
  mutate(row = row_number()) %>% 
  unnest_tokens(word, tweet_text) %>% 
  filter(!word %in% custom_stop_words$word)

tweet_cor <- tokens_2017 %>% 
  group_by(word) %>% 
  filter(n() > 500) %>% 
  pairwise_cor(word, row, sort = T)

tweet_cor_graph <- tweet_cor %>% 
  filter(correlation > .4) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

tweet_cor_graph +
  labs(title = "Words which appear most often together relative to how often they are alone")

```

###Structural topic modelling


###sampling stm

```{r}
tweets_2017_sampled <- tweets_2017 %>% 
  sample_n(25000)
```

```{r}
processed_2017 <- textProcessor(tweets_2017_sampled$tweet_text, metadata = tweets_2017_sampled)
```

```{r}
plotRemoved(processed_2017$documents, lower.thresh = seq(1, 1000, by = 100))
```




out <- prepDocuments(processed_2017$documents, processed_2017$vocab, processed_2017$meta) #run out

docs <- out$documents
vocab <- out$vocab
meta <- out$meta

stm_2017 <- stm(documents = out$documents, vocab= out$vocab, K = 0, prevalence = ~ s(tweet_time), data = out$meta, init.type = "Spectral", ngroups = 3)


plot(stm_2017, type = "summary", xlim = c(0, .2))

### Topic 22 over time


par(mar = c(10, 10, 5, 5))

out$meta$tweet_time <- as.numeric(out$meta$tweet_time)

prep <- estimateEffect(1:39 ~ s(tweet_time), stm_2017, meta = out$meta, uncertainty = "Global")

plot(prep, "tweet_time", method = "continuous", topics = c(1:4), model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2017)", ylim=  c(0.0, 0.1))
monthseq <- seq(from = as.Date("2017-01-01"), to = as.Date("2018-01-01"), by = "month")
monthnames <- months(monthseq)
axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)), labels = monthnames)


par(mfrow = c(1, 2),mar = c(.5, .5, 1, .5))

thoughts18 <- findThoughts(stm_2017, n = 3, texts =  out$meta$tweet_text, topics = 18)$docs[[1]]
thoughts15 <- findThoughts(stm_2017, n = 3, texts =  out$meta$tweet_text, topics = 15)$docs[[1]]
thoughts17 <- findThoughts(stm_2017, n = 3, texts =  out$meta$tweet_text, topics = 17)$docs[[1]]
thoughts22 <- findThoughts(stm_2017, n = 3, texts =  out$meta$tweet_text, topics = 22)$docs[[1]]

plotQuote(thoughts18, width = 30, main = "Topic 18")
plotQuote(thoughts15, width = 30, main = "Topic 15")
plotQuote(thoughts22, width = 30, main = "Topic 22")





summary(stm_2017, topics = 15)



###Russian lexicon

Make the russian sentiment lexicon and add the values

```{r}
dictionary <- read_delim("lexicon.txt", delim = ",", col_names  = F)
names(dictionary) <- c("word", "type", "word_1", "sentiment", "type_2")

dictionary_emotive <- dictionary %>% 
  select(word, sentiment) %>% 
  filter(sentiment %in% c(" negative", " positive")) %>% 
  mutate(score = ifelse(sentiment == " negative", -1, 1))

```

```{r}
ukraine <- c("Украина", "украина")

tweets_ukraine <- tweets %>% 
  filter(str_detect(tweet_text, ukraine))

tweets_ukraine_tokens <- tweets_ukraine %>% 
  mutate(rownumber = row_number()) %>% 
  unnest_tokens(word, tweet_text, token = "tweets") %>% 
  filter(!word %in% custom_stop_words$word)

tweets_ukraine_sentiment <- tweets_ukraine_tokens %>% 
  inner_join(dictionary_emotive, by = "word")

tweets_ukraine_sentiment$score <- as.numeric(tweets_ukraine_sentiment$score)

tweets_ukraine_sentiment <- tweets_ukraine_sentiment %>% 
  mutate(tweet_time = floor_date(tweet_time, unit = "1 day")) %>% 
  group_by(tweet_time) %>% 
  count(word, sentiment, score, sort = T)

tweets_ukraine_sentiment <- tweets_ukraine_sentiment %>% 
  mutate(total = score * n)

tweets_ukraine_sentiment_summary <- tweets_ukraine_sentiment %>% 
  summarise(sum = sum(total))
  
tweets_ukraine_sentiment_summary %>% 
  filter(tweet_time > as.Date("2014-08-01") & tweet_time < as.Date("2014-09-01")) %>% 
  ggplot(aes(x = tweet_time, y = sum, fill =  sum > 0)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Average sentiment per day of tweets mentioning 'Ukraine' in August 2014")



```



```{r}
tweets %>% 
  select(urls) %>% 
  na.omit() %>% 
  count(urls, sort = T)
```

```{r}
tweets %>% 
  select(urls) %>% 
  na.omit() %>% 
  filter(urls != "[]") %>% #3,197,415 rows
  unique() #1,788,930 rows
```


In this section I want to remove all unique tweets and then take only tweets that were tweeted at the same time


```{r}
unique_tweets <- tweets %>% 
  filter(!str_detect(tweet_text, '^RT')) #remove RTs

unique_tweets <- unique_tweets %>% 
  distinct(tweet_text) #identify all unique tweets 

duplicated_tweets <- tweets %>% 
  filter(!tweet_text %in% unique_tweets$tweet_text) %>%  #remove all of the tweets in the unique list from the new data frame, so we are left only with the duplicates
  filter(!str_detect(tweet_text, '^RT')) #remove RTs
  


```

There are no tweets which are just duplicates 

> look into search engine optimisation and which words appear most often with key words and phrases?
